{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a468046-61b1-4b73-9d4e-0206aa52cc1f",
   "metadata": {},
   "source": [
    "# Individual Regression Project\n",
    "\n",
    "<i>This project is an opportunity for you to put your expanded set of Python skills to use in an interesting new way. Your goal is to combine daily return data from yfinance with one or more additional data sources with the goal of estimating a regression specification that allows you to test an interesting null hypothesis. The hypothesis should be one that genuinely interests you. You will be graded on execution but not on whether any of the variables in your model are statistically significant.<i>\n",
    "\n",
    "You have a lot of discretion. In the past students have asked:\n",
    "\n",
    "- Whether/how bank stocks respond to expected and unexpected changes in interest rates by the Federal Reserve\n",
    "- Whether retail stocks have more or less volatile returns during the holiday season than non-retail stocks\n",
    "- Whether the timing/number of 8-K filings by firms predict lower or more volatile returns\n",
    "- Whether the sentiment of 8-K filings for a sample of stocks can be used to predict the level or volatility of returns\n",
    "- Whether the volume/sentiment of tweets about stocks predict future stock returns\n",
    "- Whether the excess returns of high beta stocks are higher on average than the excess returns over low beta stocks\n",
    "- Whether stocks with higher relative returns last year are more likely to have higher relative returns this year (i.e., testing for momentum)\n",
    "- Whether their clever new trading strategy generates a positive and statistically significant alpha\n",
    "\n",
    "<i>I am not looking for a long write up. I primarily want to know what regression you ran and why, what your regression output looked like, and how you interpreted the output in the context of your research question. Part 2 is intended to help me figure out if you ran the regression that you intended to run.</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a25364e-d85f-4119-8ed4-cc52937d35b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'Scott Lowder'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8add056-7fdd-4734-8589-b2641cd44883",
   "metadata": {},
   "source": [
    "### Part 1: Hypothesis (20%)\n",
    "\n",
    "In the markdown cell(s) below, please explain what hypothesis you are testing in this project and <b>why you believe that it is an interesting hypothesis to test.</b> This section should include a description of your main regression specification (and any other specifications that you believe are essential for answering your research question) and your main null hypothesis. This section should also include a description of any concerns that you might have about how to interpret the results of your regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68f4c62-15c7-43fe-ae36-de9cfc45a8d6",
   "metadata": {},
   "source": [
    "Research Question:\n",
    "Is influence of individual Robinhood investors on a stock's risk level different for small companies versus large companies?\n",
    "\n",
    "Hypothesis and Regression Specification:\n",
    "Yes, a positive change in Robinhood holdings will have a more pronounced positive effect on volatility for smaller firms.\n",
    "\n",
    "Null Hypothesis ($H_0$): The influence of Robinhood accounts changes on volatility is the same for all firms, regardless of their market size. (Key coefficient is zero.)\n",
    "\n",
    "Main Regression Specification: Pooled OLS Regression model. Main objective is to estimate the coefficient $\\beta_4$, which captures the interaction effect between Robinhood attention and firm size. Use squared returns ($R_{i,t}^2$) as the measure for next-day volatility. $$R_{i,t}^2 = \\alpha + \\beta_1 R_{i,t-1}^2 + \\beta_2 \\Delta \\text{RH}_{i,t-1} + \\beta_3 \\text{SIZE}_i + \\beta_4 (\\Delta \\text{RH}_{i,t-1} \\times \\text{SIZE}_i) + \\eta_{i,t}$$\n",
    "- $\\mathbf{R_{i,t}^2}$: Next-Day Volatility. The squared daily return, which is our proxy for how risky or volatile the stock was.\n",
    "- $\\mathbf{R_{i,t-1}^2}$: Lagged Volatility. Yesterday's volatility. This controls for the tendency of high-volatility days to follow each other.\n",
    "- $\\mathbf{\\Delta \\text{RH}_{i,t-1}}$: Robinhood Attention. The percentage change in Robinhood account holders, measured on the previous day.\n",
    "- $\\mathbf{\\text{SIZE}_i}$: Firm Size. A rank based on market capitalization, where a higher value means a relatively smaller company.\n",
    "- $\\mathbf{\\beta_4 (\\Delta \\text{RH}_{i,t-1} \\times \\text{SIZE}_i)}$: Key Test. This is the combined effect of Robinhood attention and small size. If $\\beta_4$ is positive and significant, hypothesis is supported.\n",
    "- $\\eta_{i,t}$: Error Term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f0858-43de-4999-b516-167754e29f93",
   "metadata": {},
   "source": [
    "### Part 2: Data (20%)\n",
    "\n",
    "In the coding cell(s) below, please load and process the data that you use to estimate your main regression. You do not need to include code that extracts data from yfinance (for example), but you should include enough code for me to determine what data you are working with and how you are combining it. I want to make sure that your regression is doing what you want it to be doing, but I don't expect to be able to run the cells in this section. To help with grading, please add lots of comments to your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e822c3f-4cb9-41c7-889c-66b166cabdae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 4 local tickers: ['AAPL', 'PFE', 'JPM', 'NKE']\n",
      "Fetching daily data from 2018-04-21 to 2020-08-31...\n",
      "\n",
      "Processing LOCAL RH data for 4 tickers...\n",
      "Loaded 3336 daily Robinhood observations from local files.\n",
      "\n",
      "Downloading daily price data...\n",
      "\n",
      "Fetching Market Cap data for SIZE Proxy...\n",
      "SIZE Proxy calculated for 4 firms.\n",
      "\n",
      "Final regression sample size (rows): 2296\n",
      "--- Sample of Final Regression Data (Aligned with Hypothesis) ---\n",
      "        Date Ticker      R2_t  R2_t_minus_1  Delta_RH_t_minus_1  SIZE_Proxy  \\\n",
      "0 2018-05-04   AAPL  0.001539      0.000003           -4.667573        1.00   \n",
      "1 2018-05-04    JPM  0.000123      0.000040            0.558230        0.75   \n",
      "2 2018-05-04    NKE  0.000322      0.000397            0.221298        0.25   \n",
      "3 2018-05-04    PFE  0.000005      0.000014            0.500825        0.50   \n",
      "4 2018-05-07   AAPL  0.000052      0.001539           -1.616034        1.00   \n",
      "\n",
      "   Delta_RH_x_SIZE  \n",
      "0        -4.667573  \n",
      "1         0.418673  \n",
      "2         0.055325  \n",
      "3         0.250413  \n",
      "4        -1.616034  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "from datetime import datetime, timedelta\n",
    "import statsmodels.api as sm\n",
    "import os \n",
    "\n",
    "# Sample period\n",
    "START_DATE_RH = '2018-05-01'\n",
    "END_DATE_RH = '2020-08-31'\n",
    "\n",
    "# Tickers for Analysis (Are local files)\n",
    "TICKERS = ['AAPL', 'PFE', 'JPM', 'NKE'] \n",
    "\n",
    "# Determine full period needed for yfinance data\n",
    "START_DATE_YF = (pd.to_datetime(START_DATE_RH) - timedelta(days=10)).strftime('%Y-%m-%d')\n",
    "END_DATE_YF = (pd.to_datetime(END_DATE_RH)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Working with {len(TICKERS)} local tickers: {TICKERS}\")\n",
    "print(f\"Fetching daily data from {START_DATE_YF} to {END_DATE_YF}...\")\n",
    "\n",
    "# Function to Read and Process LOCAL Robinhood Data\n",
    "def process_robinhood_data(ticker, start_date, end_date):\n",
    "    file_path = f'{ticker}.csv'\n",
    "    \n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"ERROR: Robinhood data file not found locally: {file_path}. Halting.\")\n",
    "        raise FileNotFoundError(f\"Missing required file: {file_path}\")\n",
    "        \n",
    "    try:\n",
    "        # READ FROM LOCAL FILE\n",
    "        df_rh = pd.read_csv(file_path)\n",
    "        \n",
    "        # Correct timestamp is datetime and clean data\n",
    "        df_rh['timestamp'] = pd.to_datetime(df_rh['timestamp'])\n",
    "        df_rh = df_rh.sort_values('timestamp')\n",
    "\n",
    "        # Find last observed users_holding for each day (EOD proxy)\n",
    "        eod_rh = df_rh.set_index('timestamp')['users_holding'].resample('D').last().ffill().dropna()\n",
    "        \n",
    "        # Calculate daily percentage change in users holding: Delta RH (Î”RH_t)\n",
    "        delta_rh = eod_rh.pct_change() * 100\n",
    "        \n",
    "        delta_rh = delta_rh.reset_index()\n",
    "        delta_rh.columns = ['Date', 'Delta_RH']\n",
    "        delta_rh['Ticker'] = ticker\n",
    "        \n",
    "        # Filter dates to project window\n",
    "        delta_rh = delta_rh[(delta_rh['Date'] >= start_date) & (delta_rh['Date'] <= end_date)]\n",
    "        \n",
    "        return delta_rh\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process local RH data for {ticker}. Skipping. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Process All Robinhood Tickers (LOCAL)\n",
    "print(f\"\\nProcessing LOCAL RH data for {len(TICKERS)} tickers...\")\n",
    "all_rh_data = []\n",
    "for ticker in TICKERS:\n",
    "    rh_df = process_robinhood_data(ticker, START_DATE_RH, END_DATE_RH)\n",
    "    if rh_df is not None and not rh_df.empty:\n",
    "        all_rh_data.append(rh_df)\n",
    "\n",
    "# Concatenate all individual ticker dataframes\n",
    "RH_data_panel = pd.concat(all_rh_data, ignore_index=True)\n",
    "RH_data_panel['Date'] = pd.to_datetime(RH_data_panel['Date']).dt.normalize()\n",
    "RH_data_panel = RH_data_panel.dropna(subset=['Delta_RH']).reset_index(drop=True)\n",
    "print(f\"Loaded {len(RH_data_panel)} daily Robinhood observations from local files.\")\n",
    "\n",
    "\n",
    "# Download and Process Daily Stock Price Data (Y Variable and Lagged Control)\n",
    "print(f\"\\nDownloading daily price data...\")\n",
    "prices_raw = yf.download(TICKERS, start=START_DATE_YF, end=END_DATE_YF, \n",
    "                         progress=False, auto_adjust=True)\n",
    "\n",
    "# Extract adjusted close prices and standardize format\n",
    "prices = prices_raw['Close'] if isinstance(prices_raw.columns, pd.MultiIndex) else prices_raw['Close'].to_frame(name=TICKERS[0])\n",
    "\n",
    "# Calculate daily returns (R_i,t) and squared returns (R^2_i,t)\n",
    "daily_returns = prices.pct_change().stack().reset_index()\n",
    "daily_returns.columns = ['Date', 'Ticker', 'R_t'] \n",
    "daily_returns['Date'] = pd.to_datetime(daily_returns['Date']).dt.normalize()\n",
    "daily_returns['R2_t'] = daily_returns['R_t']**2 \n",
    "\n",
    "# Calculate lagged squared return (R^2_i,t-1)\n",
    "daily_returns['R2_t_minus_1'] = daily_returns.groupby('Ticker')['R2_t'].shift(1)\n",
    "\n",
    "\n",
    "# Create SIZE Proxy (Market Capitalization Rank)\n",
    "print(\"\\nFetching Market Cap data for SIZE Proxy...\")\n",
    "market_caps = {}\n",
    "for ticker in TICKERS:\n",
    "    try:\n",
    "        info = yf.Ticker(ticker).info\n",
    "        market_cap = info.get('marketCap', np.nan) \n",
    "        market_caps[ticker] = market_cap\n",
    "    except Exception as e:\n",
    "        print(f\"Market cap fetch failed for {ticker}. Skipping from SIZE calculation.\")\n",
    "        market_caps[ticker] = np.nan\n",
    "\n",
    "size_df = pd.Series(market_caps).dropna().to_frame(name='MarketCap_USD')\n",
    "\n",
    "size_df = size_df.reset_index(names=['Ticker'])\n",
    "\n",
    "# RANKING: Assign a higher SIZE_Proxy number to a smaller firm.\n",
    "size_df['SIZE_Proxy'] = size_df['MarketCap_USD'].rank(ascending=True, method='dense') \n",
    "# Normalize SIZE proxy\n",
    "max_rank = size_df['SIZE_Proxy'].max()\n",
    "size_df['SIZE_Proxy'] = size_df['SIZE_Proxy'] / max_rank \n",
    "\n",
    "print(f\"SIZE Proxy calculated for {len(size_df)} firms.\")\n",
    "\n",
    "\n",
    "# Merge 1: Returns and RH Data (on Date and Ticker)\n",
    "regression_data = pd.merge(daily_returns, RH_data_panel, on=['Date', 'Ticker'], how='inner')\n",
    "\n",
    "# RH variable needs to be lagged by one day: Delta RH_t-1\n",
    "regression_data['Delta_RH_t_minus_1'] = regression_data.groupby('Ticker')['Delta_RH'].shift(1)\n",
    "\n",
    "# Merge 2: Add SIZE Proxy (on Ticker)\n",
    "regression_data = pd.merge(regression_data, size_df[['SIZE_Proxy', 'Ticker']], on='Ticker', how='left') \n",
    "\n",
    "# Create Interaction Term: (Delta RH_t-1 * SIZE_i)\n",
    "regression_data['Delta_RH_x_SIZE'] = regression_data['Delta_RH_t_minus_1'] * regression_data['SIZE_Proxy']\n",
    "\n",
    "# Final Cleanup\n",
    "regression_data = regression_data.dropna().reset_index(drop=True)\n",
    "\n",
    "print(f\"\\nFinal regression sample size (rows): {len(regression_data)}\")\n",
    "print(regression_data[['Date', 'Ticker', 'R2_t', 'R2_t_minus_1', 'Delta_RH_t_minus_1', 'SIZE_Proxy', 'Delta_RH_x_SIZE']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aad23b-1496-44ce-86f0-5f0827fa5d0d",
   "metadata": {},
   "source": [
    "### Part 3: Regression Specification(s) (30%)\n",
    "\n",
    "In the coding cell(s) below, please estimate and report your main regression specification using the .summary() option (adding useful variable names). To help me replicate your main regression specification, I will need your column of y values and your matrix of X values. Please save those objects as 'my_y.npy' and 'my_X.npy' and upload them to Canvas along with this notebook. If you are analyzing time-series data (and I expect that virtually all of you are) I would also like the corresponding list of dates stored as 'my_dates.npy'.\n",
    "\n",
    "Note: If you choose to estimate one or more additional regression specifications, please clearly state what each of these specifications allows you to learn that your main specification does not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbdca878-6c7e-47b4-b0cb-5a4442f6efd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved required .npy files: my_y.npy, my_X.npy, my_dates.npy\n",
      "\n",
      "Estimating Pooled OLS Regression...\n",
      "\n",
      "================================================================================\n",
      "MAIN REGRESSION: RH Attention Predicting Volatility with Size Interaction\n",
      "Dependent Variable: Daily Squared Return (R2_t, Proxy for Volatility)\n",
      "Null Hypothesis (H0): Coefficient on Delta_RH_x_SIZE is zero (Beta_4 = 0)\n",
      "SIZE_Proxy: Higher value means relatively smaller firm.\n",
      "================================================================================\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                   R2_t   R-squared:                       0.195\n",
      "Model:                            OLS   Adj. R-squared:                  0.194\n",
      "Method:                 Least Squares   F-statistic:                     138.8\n",
      "Date:                Sat, 13 Dec 2025   Prob (F-statistic):          2.51e-106\n",
      "Time:                        17:20:19   Log-Likelihood:                 11833.\n",
      "No. Observations:                2296   AIC:                        -2.366e+04\n",
      "Df Residuals:                    2291   BIC:                        -2.363e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "===================================================================================\n",
      "                      coef    std err          t      P>|t|      [0.025      0.975]\n",
      "-----------------------------------------------------------------------------------\n",
      "Intercept           0.0002   7.34e-05      2.829      0.005    6.36e-05       0.000\n",
      "R2_t-1              0.3904      0.020     19.934      0.000       0.352       0.429\n",
      "Delta_RH_t-1    -7.172e-05    4.4e-05     -1.631      0.103      -0.000    1.45e-05\n",
      "SIZE_Proxy       1.813e-05      0.000      0.168      0.866      -0.000       0.000\n",
      "Delta_RH_x_SIZE     0.0004   8.74e-05      4.071      0.000       0.000       0.001\n",
      "==============================================================================\n",
      "Omnibus:                     3388.230   Durbin-Watson:                   1.366\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          1580046.204\n",
      "Skew:                           8.690   Prob(JB):                         0.00\n",
      "Kurtosis:                     130.335   Cond. No.                     1.21e+03\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.21e+03. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define Y (Dependent Variable: R^2_i,t - Daily Squared Return)\n",
    "Y = regression_data['R2_t']\n",
    "\n",
    "# Define X (Independent Variables from hypothesis)\n",
    "# X_vars: R2_t-1, Delta_RH_t-1, SIZE_Proxy, Delta_RH_x_SIZE\n",
    "X_vars = ['R2_t_minus_1', 'Delta_RH_t_minus_1', 'SIZE_Proxy', 'Delta_RH_x_SIZE']\n",
    "X = regression_data[X_vars]\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "my_y = Y.to_numpy()\n",
    "my_X = X.to_numpy()\n",
    "\n",
    "# Create date array\n",
    "my_dates = (\n",
    "    regression_data[['Date', 'Ticker']]\n",
    "    .drop_duplicates()\n",
    "    .sort_values(['Ticker', 'Date'])\n",
    "    ['Date']\n",
    "    .dt.strftime('%Y-%m-%d')\n",
    "    .to_numpy()\n",
    ")\n",
    "\n",
    "# Save files\n",
    "np.save('my_y.npy', my_y)\n",
    "np.save('my_X.npy', my_X)\n",
    "np.save('my_dates.npy', my_dates)\n",
    "\n",
    "# Estimate Pooled OLS Regression\n",
    "print(\"\\nEstimating Pooled OLS Regression...\")\n",
    "model = sm.OLS(Y, X)\n",
    "main_regression_result = model.fit()\n",
    "\n",
    "# Output\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MAIN REGRESSION: RH Attention Predicting Volatility with Size Interaction\")\n",
    "print(\"Dependent Variable: Daily Squared Return (R2_t, Proxy for Volatility)\")\n",
    "print(\"Null Hypothesis (H0): Coefficient on Delta_RH_x_SIZE is zero (Beta_4 = 0)\")\n",
    "print(\"SIZE_Proxy: Higher value means relatively smaller firm.\")\n",
    "print(\"=\"*80)\n",
    "print(main_regression_result.summary(\n",
    "    xname=['Intercept', 'R2_t-1', 'Delta_RH_t-1', 'SIZE_Proxy', 'Delta_RH_x_SIZE']\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087c473-9e8c-449b-84a7-7a665171930b",
   "metadata": {},
   "source": [
    "### Part 4: Interpretation and Conclusion (30%)\n",
    "\n",
    "In the markdown cell(s) below, please explain whether your regression caused you to accept or reject your null hypothesis, and the confidence level used to assess statistical significant. Please discuss what you learned from running this regression and what additional regressions you might want to run in the future (potentially using additional data) to learn more about your research question. <i>I will be happy with one very good paragraph.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79ee1d-3de1-4dd0-8870-45ec3fc48df4",
   "metadata": {},
   "source": [
    "Based on the results from the regression, reject the null hypothesis that the coefficient on the interaction term ($\\Delta \\text{RH}_{i,t-1} \\times \\text{SIZE}_i$) is zero, using the standard 99% confidence level (since the P-value was $0.000$). The positive and highly significant coefficient on the interaction term ($\\beta_4=0.0004$) says that the predictive power of increased Robinhood attention on next-day stock volatility is significantly greater for smaller firms (those with a higher $\\text{SIZE}$ proxy value) than for larger firms, validating the research hypothesis that retail trading disproportionately impacts the market behavior of smaller stocks. Moving forward, to further explore this phenomenon, future research should incorporate firm fixed effects to control for unobserved, time-invariant differences between companies (like brand value or risk tolerance) and could also be expanded to include data on short interest as a control variable, testing whether the \"Robinhood effect\" is distinct from general speculative short squeezes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
